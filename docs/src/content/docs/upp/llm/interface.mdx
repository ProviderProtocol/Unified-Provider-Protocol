---
title: LLM Interface
description: UPP-1.3 LLM Interface specification for chat and completion.
---

import { Aside, Badge, Card, CardGrid } from '@astrojs/starlight/components';

# LLM Interface

<Badge text="UPP-1.3.0" variant="note" />

## Function Signature

```
llm(options: LLMOptions) -> LLMInstance
```

## LLMOptions

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `model` | ModelReference | Yes | Model reference from provider factory |
| `config` | ProviderConfig | No | Provider configuration |
| `params` | Map | No | Model-specific parameters |
| `system` | String | No | System prompt |
| `tools` | List&lt;Tool&gt; | No | Available tools |
| `toolStrategy` | ToolUseStrategy | No | Tool execution strategy |
| `structure` | JSONSchema | No | Structured output schema |

## LLMInstance

| Method/Property | Description |
|-----------------|-------------|
| `generate(...)` | Execute inference, return complete Turn |
| `stream(...)` | Execute streaming inference |
| `model` | The bound model |
| `capabilities` | Provider API capabilities |

**generate() Signatures:**

```
generate(input: InferenceInput) -> Promise&lt;Turn&gt;
generate(...inputs: InferenceInput[]) -> Promise&lt;Turn&gt;
generate(history: List&lt;Message&gt;, ...inputs: InferenceInput[]) -> Promise&lt;Turn&gt;
```

**InferenceInput:** `String | Message | ContentBlock`

## LLMCapabilities

Capabilities declare what the **provider's API** supports, not individual model capabilities:

| Field | Type | Description |
|-------|------|-------------|
| `streaming` | Boolean | Supports streaming responses |
| `tools` | Boolean | Supports tool/function calling |
| `structuredOutput` | Boolean | Supports native structured output |
| `imageInput` | Boolean | Supports image input |
| `documentInput` | Boolean | Supports document input |
| `videoInput` | Boolean | Supports video input |
| `audioInput` | Boolean | Supports audio input |

Capabilities are static for the lifetime of a provider instance.

## Tool Execution Loop

The `llm()` core manages the tool execution loop. Providers only handle single request/response cycles.

**Loop Flow:**

1. Convert input to UserMessage, append to messages
2. Call provider's `complete()` method
3. If response has tool calls AND iterations < maxIterations:
   - Execute tools (parallel if multiple)
   - Append AssistantMessage and ToolResultMessage
   - Return to step 2
4. Build Turn from accumulated messages

## Provider Responsibilities

Providers MUST:

1. **Transform requests:** Convert UPP structures to vendor format
2. **Transform responses:** Map vendor responses to `AssistantMessage`
3. **Handle system prompts:** Transform to vendor-specific format
4. **Normalize errors:** Wrap vendor errors in `UPPError`
5. **Namespace metadata:** Store vendor-specific data under `metadata.{providerName}`